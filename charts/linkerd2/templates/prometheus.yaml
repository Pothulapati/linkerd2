{{with .Values -}}
---
###
### Prometheus
###
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: linkerd-prometheus-config
  namespace: {{.global.Namespace}}
  labels:
    {{.global.ControllerComponentLabel}}: prometheus
    {{.global.ControllerNamespaceLabel}}: {{.global.Namespace}}
  annotations:
    {{.global.CreatedByAnnotation}}: {{default (printf "linkerd/helm %s" .global.LinkerdVersion) .CliVersion}}
data:
  prometheus.yml: |-
    global:
      scrape_interval: 10s
      scrape_timeout: 10s
      evaluation_interval: 10s

    rule_files:
    - /etc/prometheus/*_rules.yml

    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']

    - job_name: 'grafana'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['{{.global.Namespace}}']
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_container_name
        action: keep
        regex: ^grafana$

    #  Required for: https://grafana.com/grafana/dashboards/315
    - job_name: 'kubernetes-nodes-cadvisor'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
      metric_relabel_configs:
      - source_labels: [__name__]
        regex: '(container|machine)_(cpu|memory|network|fs)_(.+)'
        action: keep
      - source_labels: [__name__]
        regex: 'container_memory_failures_total' # unneeded large metric
        action: drop

    - job_name: 'linkerd-controller'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['{{.global.Namespace}}']
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_label_linkerd_io_control_plane_component
        - __meta_kubernetes_pod_container_port_name
        action: keep
        regex: (.*);admin-http$
      - source_labels: [__meta_kubernetes_pod_container_name]
        action: replace
        target_label: component

    - job_name: 'linkerd-proxy'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_container_name
        - __meta_kubernetes_pod_container_port_name
        - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
        action: keep
        regex: ^{{default .ProxyContainerName "linkerd-proxy" .ProxyContainerName}};linkerd-admin;{{.global.Namespace}}$
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod
      # special case k8s' "job" label, to not interfere with prometheus' "job"
      # label
      # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
      # k8s_job=foo
      - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job]
        action: replace
        target_label: k8s_job
      # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
      - action: labeldrop
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
      # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
      # deployment=foo
      - action: labelmap
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
      # drop all labels that we just made copies of in the previous labelmap
      - action: labeldrop
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
      # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
      # foo=bar
      - action: labelmap
        regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
---
kind: Service
apiVersion: v1
metadata:
  name: linkerd-prometheus
  namespace: {{.global.Namespace}}
  labels:
    {{.global.ControllerComponentLabel}}: prometheus
    {{.global.ControllerNamespaceLabel}}: {{.global.Namespace}}
  annotations:
    {{.global.CreatedByAnnotation}}: {{default (printf "linkerd/helm %s" .global.LinkerdVersion) .CliVersion}}
spec:
  type: ClusterIP
  selector:
    {{.global.ControllerComponentLabel}}: prometheus
  ports:
  - name: admin-http
    port: 9090
    targetPort: 9090
---
{{ if empty .global.Proxy.Image.Version -}}
{{ $_ := set .global.Proxy.Image "Version" .global.LinkerdVersion -}}
{{ end -}}
{{ $_ := set .global.Proxy "WorkloadKind" "deployment" -}}
{{ $_ := set .global.Proxy "Component" "linkerd-prometheus" -}}
{{ include "linkerd.proxy.validation" .global.Proxy -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    {{.global.CreatedByAnnotation}}: {{default (printf "linkerd/helm %s" .global.LinkerdVersion) .CliVersion}}
  labels:
    {{.global.ControllerComponentLabel}}: prometheus
    {{.global.ControllerNamespaceLabel}}: {{.global.Namespace}}
  name: linkerd-prometheus
  namespace: {{.global.Namespace}}
spec:
  replicas: 1
  selector:
    matchLabels:
      {{.global.ControllerComponentLabel}}: prometheus
      {{.global.ControllerNamespaceLabel}}: {{.global.Namespace}}
      {{- include "partials.proxy.labels" .global.Proxy | nindent 6}}
  template:
    metadata:
      annotations:
        {{.global.CreatedByAnnotation}}: {{default (printf "linkerd/helm %s" .global.LinkerdVersion) .CliVersion}}
        {{- include "partials.proxy.annotations" .global.Proxy| nindent 8}}
      labels:
        {{.global.ControllerComponentLabel}}: prometheus
        {{.global.ControllerNamespaceLabel}}: {{.global.Namespace}}
        {{- include "partials.proxy.labels" .global.Proxy | nindent 8}}
    spec:
      {{- include "linkerd.node-selector" . | nindent 6 }}
      containers:
      - args:
        - --storage.tsdb.path=/data
        - --storage.tsdb.retention.time=6h
        - --config.file=/etc/prometheus/prometheus.yml
        - --log.level={{lower .PrometheusLogLevel}}
        image: {{.PrometheusImage}}
        imagePullPolicy: {{.global.ImagePullPolicy}}
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        name: prometheus
        ports:
        - containerPort: 9090
          name: admin-http
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        {{- if .PrometheusResources -}}
        {{- include "partials.resources" .PrometheusResources | nindent 8 }}
        {{- end }}
        securityContext:
          runAsUser: 65534
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /etc/prometheus
          name: prometheus-config
          readOnly: true
      - {{- include "partials.proxy" . | indent 8 | trimPrefix (repeat 7 " ") }}
      {{ if not .NoInitContainer -}}
      initContainers:
      - {{- include "partials.proxy-init" . | indent 8 | trimPrefix (repeat 7 " ") }}
      {{ end -}}
      serviceAccountName: linkerd-prometheus
      volumes:
      - emptyDir: {}
        name: data
      - configMap:
          name: linkerd-prometheus-config
        name: prometheus-config
      - {{- include "partials.proxy.volumes.identity" . | indent 8 | trimPrefix (repeat 7 " ") }}
{{- end }}
